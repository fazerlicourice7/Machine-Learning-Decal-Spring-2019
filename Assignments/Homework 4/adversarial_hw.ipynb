{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Examples Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Do not be intimidated by the large chunks of code, you do not need to understand all of it to do the homework. As long as you understand how adversarial examples work, to fill in the homework you just need to understand what the comment above the FILL_IN is about)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will learn about adversarial examples. Adversarial examples are deliberately created inputs that fool a neural network. For example, in the picture below, by adding noise to an image of a panda that is correctly classified as a panda, we can fool a neural network into thinking it is a gibbon.\n",
    "\n",
    "![](./images/panda.png)\n",
    "\n",
    "Clearly, this has very dangerous consequences, especially considering how prevalent computer vision classification systems are. For example, in the image below physical stickers have been strategically placed on a stop sign that tricks a self-driving car into thinking it is a different traffic sign.\n",
    "\n",
    "![](./images/stop_sign.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below, it contains helper functions that will be used. You do not need to understand the details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(cv2im, resize_im=True):\n",
    "    \"\"\"\n",
    "        Processes image for CNNs\n",
    "\n",
    "    Args:\n",
    "        PIL_img (PIL_img): Image to process\n",
    "        resize_im (bool): Resize to 224 or not\n",
    "    returns:\n",
    "        im_as_var (Pytorch variable): Variable that contains processed float tensor\n",
    "    \"\"\"\n",
    "    # mean and std list for channels (Imagenet)\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    # Resize image\n",
    "    if resize_im:\n",
    "        cv2im = cv2.resize(cv2im, (224, 224))\n",
    "    im_as_arr = np.float32(cv2im)\n",
    "    im_as_arr = np.ascontiguousarray(im_as_arr[..., ::-1])\n",
    "    im_as_arr = im_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n",
    "    # Normalize the channels\n",
    "    for channel, _ in enumerate(im_as_arr):\n",
    "        im_as_arr[channel] /= 255\n",
    "        im_as_arr[channel] -= mean[channel]\n",
    "        im_as_arr[channel] /= std[channel]\n",
    "    # Convert to float tensor\n",
    "    im_as_ten = torch.from_numpy(im_as_arr).float()\n",
    "    # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n",
    "    im_as_ten.unsqueeze_(0)\n",
    "    # Convert to Pytorch variable\n",
    "    im_as_var = Variable(im_as_ten, requires_grad=True)\n",
    "    return im_as_var\n",
    "\n",
    "\n",
    "def recreate_image(im_as_var):\n",
    "    \"\"\"\n",
    "        Recreates images from a torch variable, sort of reverse preprocessing\n",
    "\n",
    "    Args:\n",
    "        im_as_var (torch variable): Image to recreate\n",
    "\n",
    "    returns:\n",
    "        recreated_im (numpy arr): Recreated image in array\n",
    "    \"\"\"\n",
    "    reverse_mean = [-0.485, -0.456, -0.406]\n",
    "    reverse_std = [1/0.229, 1/0.224, 1/0.225]\n",
    "    recreated_im = copy.copy(im_as_var.data.numpy()[0])\n",
    "    for c in range(3):\n",
    "        recreated_im[c] /= reverse_std[c]\n",
    "        recreated_im[c] -= reverse_mean[c]\n",
    "    recreated_im[recreated_im > 1] = 1\n",
    "    recreated_im[recreated_im < 0] = 0\n",
    "    recreated_im = np.round(recreated_im * 255)\n",
    "\n",
    "    recreated_im = np.uint8(recreated_im).transpose(1, 2, 0)\n",
    "    # Convert RBG to GBR\n",
    "    recreated_im = recreated_im[..., ::-1]\n",
    "    return recreated_im\n",
    "\n",
    "\n",
    "def get_params(example_index):\n",
    "    \"\"\"\n",
    "        Gets used variables for almost all visualizations, like the image, model etc.\n",
    "\n",
    "    Args:\n",
    "        example_index (int): Image id to use from examples\n",
    "\n",
    "    returns:\n",
    "        original_image (numpy arr): Original image read from the file\n",
    "        prep_img (numpy_arr): Processed image\n",
    "        target_class (int): Target class for the image\n",
    "        file_name_to_export (string): File name to export the visualizations\n",
    "        pretrained_model(Pytorch model): Model to use for the operations\n",
    "    \"\"\"\n",
    "    # Pick one of the examples\n",
    "    example_list = [['./input_images/apple.JPEG', 948],\n",
    "                    ['./input_images/eel.JPEG', 390],\n",
    "                    ['./input_images/bird.JPEG', 13]]\n",
    "    selected_example = example_index\n",
    "    img_path = example_list[selected_example][0]\n",
    "    target_class = example_list[selected_example][1]\n",
    "    file_name_to_export = img_path[img_path.rfind('/')+1:img_path.rfind('.')]\n",
    "    # Read image\n",
    "    original_image = cv2.imread(img_path, 1)\n",
    "    # Process image\n",
    "    prep_img = preprocess_image(original_image)\n",
    "    # Define model\n",
    "    pretrained_model = models.alexnet(pretrained=True)\n",
    "    return (img_path, original_image,\n",
    "            prep_img,\n",
    "            target_class,\n",
    "            file_name_to_export,\n",
    "            pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will explore the Fast Gradient Sign Method (FGSM) of generating adversarial examples. You can find the paper that introduces it at this link if you are interested in reading more about it: https://arxiv.org/abs/1412.6572\n",
    "\n",
    "The idea behind this method is in a sense the reversal of backpropagation. This is because we want to calculate the gradient of the cost (same as backprop so far) with respect to the input image pixels (different since with respect to model weights for backprop). In a sense, we want to know how perturbing each pixel will affect the cost, which will in turn affect what label the machine learning model classifies an image as. Once we know this, we know exactly how to exploit and perturb the image the minimal amount in order to get the model to classify it as a different label. Mathematically, FGSM takes the gradient computed from the description above, and converts each number into either +1 or -1 depending on its sign. Then, it multiplies this by a very small epsilon value and adds it to the original image. This new resulting image will then be classified incorrectly by the model this attack was created against. Pretty terrifying how easy it is to fool these models, huh.\n",
    "\n",
    "The loss function used in this attack is the cross entropy loss between what the model predicts, and the actual label used. To get the label the model predicts that the loss is run on, you will need to actually call the model on the input image. The result of this is what is used for the loss function described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast Gradient Sign Untargeted to Fill In \n",
    "\n",
    "class FastGradientSignUntargeted():\n",
    "    \"\"\"\n",
    "        Fast gradient sign untargeted adversarial attack, minimizes the initial class activation\n",
    "        with iterative grad sign updates\n",
    "    \"\"\"\n",
    "    def __init__(self, model, alpha):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        # Movement multiplier per iteration\n",
    "        self.alpha = alpha\n",
    "        # Create the folder to export images if not exists\n",
    "        if not os.path.exists('./generated'):\n",
    "            os.makedirs('./generated')\n",
    "\n",
    "    def generate(self, original_image, im_label):\n",
    "        # image label as variable\n",
    "        im_label_as_var = Variable(torch.from_numpy(np.asarray([im_label])))\n",
    "        # Define loss functions\n",
    "        ce_loss = nn.CrossEntropyLoss()\n",
    "        # Process image\n",
    "        processed_image = preprocess_image(original_image)\n",
    "        # Start iteration\n",
    "        for i in range(10):\n",
    "            print('Iteration:', str(i))\n",
    "            # zero_gradients(x)\n",
    "            # Zero out previous gradients\n",
    "            # Can also use zero_gradients(x)\n",
    "            processed_image.grad = None\n",
    "            # Forward pass\n",
    "            model_output = self.model(processed_image)\n",
    "            # Calculate CE loss\n",
    "            pred_loss = ce_loss(model_output, im_label_as_var.long())\n",
    "            # Do backward pass\n",
    "            pred_loss.backward()\n",
    "            # Create Noise\n",
    "            # Here, processed_image.grad.data is also the same thing is the backward gradient from\n",
    "            # the first layer, can use that with hooks as well\n",
    "            adv_noise = self.alpha * torch.sign(processed_image.grad.data)\n",
    "            # Add Noise to processed image\n",
    "            processed_image.data = processed_image + adv_noise\n",
    "\n",
    "            # Confirming if the image is indeed adversarial with added noise\n",
    "            # This is necessary (for some cases) because when we recreate image\n",
    "            # the values become integers between 1 and 255 and sometimes the adversariality\n",
    "            # is lost in the recreation process\n",
    "\n",
    "            # Generate confirmation image\n",
    "            recreated_image = recreate_image(processed_image)\n",
    "            # Process confirmation image\n",
    "            prep_confirmation_image = preprocess_image(recreated_image)\n",
    "            # Forward pass to make sure creating the adversarial example was successful\n",
    "            confirmation_out = self.model(prep_confirmation_image)\n",
    "            # Get prediction\n",
    "            _, confirmation_prediction = confirmation_out.data.max(1)\n",
    "            # Get Probability\n",
    "            confirmation_confidence = \\\n",
    "                nn.functional.softmax(confirmation_out)[0][confirmation_prediction].data.numpy()[0]\n",
    "            # Convert tensor to int\n",
    "            confirmation_prediction = confirmation_prediction.numpy()[0]\n",
    "            # Check if the prediction is different than the original\n",
    "            if confirmation_prediction != im_label:\n",
    "                print('Original image was predicted as:', im_label,\n",
    "                      'with adversarial noise converted to:', confirmation_prediction,\n",
    "                      'and predicted with confidence of:', confirmation_confidence)\n",
    "                # Create the image for noise, which is the difference between the\n",
    "                # adversarial example and original image\n",
    "                noise_image = recreated_image - prep_confirmation_image\n",
    "                name_noise = './generated/untargeted_adv_noise_from_' + str(im_label) + '_to_' + str(confirmation_prediction) + '.jpg'\n",
    "                cv2.imwrite(name_noise, noise_image)\n",
    "                # Write image\n",
    "                name_image = './generated/untargeted_adv_img_from_' + str(im_label) + '_to_' + str(confirmation_prediction) + '.jpg'\n",
    "                cv2.imwrite(name_image, recreated_image)\n",
    "                \n",
    "                \n",
    "                return name_noise, name_image\n",
    "\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test your implementation by running the below cell and visualizing. The first image is the original input image, second image is the noise added, and third is the adversarially perturbed image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vamsh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image was predicted as: 13 with adversarial noise converted to: 19 and predicted with confidence of: 0.9689029\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "rsub() received an invalid combination of arguments - got (Tensor, numpy.ndarray), but expected one of:\n * (Tensor input, Tensor other, Number alpha)\n * (Tensor input, Number other, Number alpha)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-67979c1c3c15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mFGS_untargeted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFastGradientSignUntargeted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mname_noise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFGS_untargeted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0moriginal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-297057bcfeae>\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, original_image, im_label)\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[1;31m# Create the image for noise, which is the difference between the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[1;31m# adversarial example and original image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[0mnoise_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecreated_image\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mprep_confirmation_image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m                 \u001b[0mname_noise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./generated/untargeted_adv_noise_from_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim_label\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_to_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfirmation_prediction\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.jpg'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_noise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vamsh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__rsub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__rsub__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__rdiv__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: rsub() received an invalid combination of arguments - got (Tensor, numpy.ndarray), but expected one of:\n * (Tensor input, Tensor other, Number alpha)\n * (Tensor input, Number other, Number alpha)\n"
     ]
    }
   ],
   "source": [
    "target_example = 2\n",
    "(img_path, original_image, prep_img, target_class, _, pretrained_model) =\\\n",
    "    get_params(target_example)\n",
    "\n",
    "FGS_untargeted = FastGradientSignUntargeted(pretrained_model, 0.01)\n",
    "name_noise, name_image = FGS_untargeted.generate(original_image, target_class)\n",
    "\n",
    "original = Image(img_path)\n",
    "noise = Image(name_noise)\n",
    "adversarial = Image(name_image)\n",
    "\n",
    "display(original, noise, adversarial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you did above was an untargeted FGSM, so what it did was make a small change to fool the classifier into thinking the image was labeled with any other label than the one it actually is. On the other hand, ~targeted~ FGSM is where the adversarial image that is created is of a very deliberate, specific label. Besides that, the same techniques are applied. Read through the code below and fill in the 5 blank spots with very similar code to what you filled in above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastGradientSignTargeted():\n",
    "    \"\"\"\n",
    "        Fast gradient sign untargeted adversarial attack, maximizes the target class activation\n",
    "        with iterative grad sign updates\n",
    "    \"\"\"\n",
    "    def __init__(self, model, alpha):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        # Movement multiplier per iteration\n",
    "        self.alpha = alpha\n",
    "        # Create the folder to export images if not exists\n",
    "        if not os.path.exists('./generated'):\n",
    "            os.makedirs('./generated')\n",
    "\n",
    "    def generate(self, original_image, org_class, target_class):\n",
    "        # I honestly dont know a better way to create a variable with specific value\n",
    "        # Targeting the specific class\n",
    "        im_label_as_var = Variable(torch.from_numpy(np.asarray([target_class])))\n",
    "        # Define loss functions\n",
    "        ce_loss = nn.CrossEntropyLoss()\n",
    "        # Process image\n",
    "        processed_image = preprocess_image(original_image)\n",
    "        # Start iteration\n",
    "        for i in range(10):\n",
    "            print('Iteration:', str(i))\n",
    "            # zero_gradients(x)\n",
    "            # Zero out previous gradients\n",
    "            # Can also use zero_gradients(x)\n",
    "            processed_image.grad = None\n",
    "            # Forward pass\n",
    "            model_output = self.model(processed_image)\n",
    "            # Calculate CE loss\n",
    "            pred_loss = ce_loss(model_output, im_label_as_var.long())\n",
    "            # Do backward pass\n",
    "            pred_loss.backward()\n",
    "            # Create Noise\n",
    "            # Here, processed_image.grad.data is also the same thing is the backward gradient from\n",
    "            # the first layer, can use that with hooks as well\n",
    "            adv_noise = self.alpha * torch.sign(processed_image.grad.data)\n",
    "            # Subtract noise to processed image\n",
    "            processed_image.data = processed_image + adv_noise\n",
    "\n",
    "            # Confirming if the image is indeed adversarial with added noise\n",
    "            # This is necessary (for some cases) because when we recreate image\n",
    "            # the values become integers between 1 and 255 and sometimes the adversariality\n",
    "            # is lost in the recreation process\n",
    "\n",
    "            # Generate confirmation image\n",
    "            recreated_image = recreate_image(processed_image)\n",
    "            # Process confirmation image\n",
    "            prep_confirmation_image = preprocess_image(recreated_image)\n",
    "            # Forward pass\n",
    "            confirmation_out = self.model(prep_confirmation_image)\n",
    "            # Get prediction\n",
    "            _, confirmation_prediction = confirmation_out.data.max(1)\n",
    "            # Get Probability\n",
    "            confirmation_confidence = \\\n",
    "                nn.functional.softmax(confirmation_out)[0][confirmation_prediction].data.numpy()[0]\n",
    "            # Convert tensor to int\n",
    "            confirmation_prediction = confirmation_prediction.numpy()[0]\n",
    "            # Check if the prediction is different than the original\n",
    "            if confirmation_prediction == target_class:\n",
    "                print('Original image was predicted as:', org_class,\n",
    "                      'with adversarial noise converted to:', confirmation_prediction,\n",
    "                      'and predicted with confidence of:', confirmation_confidence)\n",
    "                # Create the image for noise as: Original image - generated image\n",
    "                noise_image = processed_image - prep_confirmation_image\n",
    "                name_noise = './generated/targeted_adv_noise_from_' + str(org_class) + '_to_' + str(confirmation_prediction) + '.jpg'\n",
    "                cv2.imwrite(name_noise, noise_image)\n",
    "                # Write image\n",
    "                name_image = './generated/targeted_adv_img_from_' + str(org_class) + '_to_' + str(confirmation_prediction) + '.jpg'\n",
    "                cv2.imwrite(name_image, recreated_image)\n",
    "                return name_noise, name_image\n",
    "                break\n",
    "\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vamsh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:58: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Iteration: 9\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-fbb47be1eb07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mFGS_untargeted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFastGradientSignTargeted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mname_noise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFGS_untargeted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morg_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0moriginal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "target_example = 0  # Apple\n",
    "(img_path, original_image, prep_img, org_class, _, pretrained_model) =\\\n",
    "    get_params(target_example)\n",
    "target_class = 62  # Mud turtle\n",
    "\n",
    "FGS_untargeted = FastGradientSignTargeted(pretrained_model, 0.01)\n",
    "name_noise, name_image = FGS_untargeted.generate(original_image, org_class, target_class)\n",
    "\n",
    "original = Image(img_path)\n",
    "noise = Image(name_noise)\n",
    "adversarial = Image(name_image)\n",
    "\n",
    "display(original, noise, adversarial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
