{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning at Berkeley: Machine Learning Decal\n",
    "## Homework Three: Unsupervised Learning and Autoencoders\n",
    "\n",
    "Release Date: February 27th, 2019\n",
    "Due Date: March 11th, 2019\n",
    "Contributing Authors: Brandon Trabucco\n",
    "\n",
    "The goal of this homework is to familiarize you with various unsupervised learning and dimensionality reduction algorithms that are commonly used when handling large datasets. In particular, you will implement:\n",
    "\n",
    "* Extracting The Dataset\n",
    "* Principal Component Analysis\n",
    "* A Linear Autoencoder\n",
    "* A Convolutional Autoencoder\n",
    "* (Optional) A Variational Autoencoder (VAE for short)\n",
    "\n",
    "In addition to implementing these algorithms, you will use these algorithms to interpolate between existing data points, and extrapolate to new data points. Since images have nice visualizations, this homework shall use a miniature version of the CelebA (S. Yang et al. 2015) dataset that contains 5000 cropped images of celebrity faces. Feel free to download the full dataset after finishing the homework and tinkering with your models.\n",
    "\n",
    "S. Yang, P. Luo, C. C. Loy, and X. Tang, \"From Facial Parts Responses to Face Detection: A Deep Learning Approach\", in IEEE International Conference on Computer Vision (ICCV), 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# IMPORTANT: you must have all of these repositories properly installed on your machine to complete tis homework.\n",
    "# you must also have ffmpeg installed. You may find the binaries at https://www.ffmpeg.org/download.html\n",
    "# Make sure you add the directories that contain the ffmpeg binaries to your path, reinstall matplotlib afterwards\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "if not \"ffmpeg\" in matplotlib.animation.writers.list():\n",
    "    print(\"WARNING!!! You must add FFMPEG to your path before you can use the animations in this homework.\")\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section One: Extracting The Dataset\n",
    "\n",
    "In this section, you will extract the folder of images into a matrix $X \\in \\mathcal{R}^{N \\times D}$ where the number of rows $N$ corresponds to the number of images in the dataset (5000 in total), and the number of features $D$ corresponds to the RGB values of every pixel in every image (32 * 32 * 3 = 3072 in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataset(path_to_images, output_height, output_width, path_to_matrix):\n",
    "    \"\"\"Loads each image into memory, processes each image, and saves a matrix to the disk.\n",
    "    Args:\n",
    "        path_to_images: string, the path to the directory containing image files.\n",
    "        output_height: integer, the height to scale each image to.\n",
    "        output_width: integer, the width to scale each image to.\n",
    "        path_to_matrix: string, the path where the matrix will be saved.\n",
    "    \"\"\"\n",
    "    all_matching_files = glob.glob(os.path.join(path_to_images, \"*.jpg\"))\n",
    "    X = np.zeros([len(all_matching_files), output_height * output_width * 3])\n",
    "    for i, file in enumerate(all_matching_files):\n",
    "        # TODO: fill in this section to accomplish the following.\n",
    "        # 1) load the image with Image.open specified by its file path from the disk\n",
    "        # 2) resize that image to be a [output_width, output_height] numpy array\n",
    "        # 3) perform a row-major flatten of the array\n",
    "        # 3) scale the elements of the array to be in the range [-1, 1]\n",
    "        # 4) assign the arrya to the ith column of data matrix X\n",
    "        # BEGIN YOUR CODE\n",
    "        image = Image.open(file)\n",
    "        image = image.resize((output_width, output_height))\n",
    "        image.load()\n",
    "        image = np.asarray(image, dtype=np.float32).reshape([output_height * output_width * 3])        \n",
    "        #image = image.flatten()\n",
    "        image = (image - 127) / 127\n",
    "        X[i] = image\n",
    "        # END YOUR CODE\n",
    "    np.save(os.path.join(path_to_matrix, \"dataset.npy\"), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path_to_matrix):\n",
    "    \"\"\"Loads a matrix containing processed images into the memory.\n",
    "    Args:\n",
    "        path_to_matrix: string, the path where the matrix was saved.\n",
    "    Returns:\n",
    "        a numpy matrix with 5000 rows (one per image) and 3072 columns.\n",
    "    \"\"\"\n",
    "    return np.load(os.path.join(path_to_matrix, \"dataset.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(flat_image_vector, output_height, output_width):\n",
    "    \"\"\"Displays an image on jupyter notebook using matplotlib imshow.\n",
    "    Args:\n",
    "        flat_image_vector: a np.float32 vector with D = 3072 elements.\n",
    "        output_height: integer, the height to reshape the image to.\n",
    "        output_width: integer, the width to reshape the image to.\n",
    "    \"\"\"\n",
    "    # TODO: fill in this section to accomplish the following.\n",
    "    # 1) perform a row-major reshape from a flattened array to a [output_height, output_width, 3] tensor\n",
    "    # 2) scale the elements of the array to be in the range [0, 1]\n",
    "    # 3) render the image using matplotlib imshow(...)\n",
    "    # 4) show() and close() the plot\n",
    "    # BEGIN YOUR CODE\n",
    "    image_tensor = np.reshape(flat_image_vector, (output_height, output_width, 3))\n",
    "    image_tensor = np.interp(image_tensor, (image_tensor.min(), image_tensor.max()), (0,1))\n",
    "    #image_tensor += 1\n",
    "    #image_tensor /= 2\n",
    "    plt.imshow(image_tensor)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    # END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in this section to accomplish the following.\n",
    "# 1) call the extract_dataset function with the appropriate paths\n",
    "# 2) assign the height 32 and the width 32\n",
    "# BEGIN YOUR CODE\n",
    "extract_dataset(\"../../../celeba/\", 32, 32, \"../../../\")\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A matrix with 5000 images and 3072 features per image was loaded.\n"
     ]
    }
   ],
   "source": [
    "X = None\n",
    "# TODO: fill in this section to accomplish the following.\n",
    "# 1) call the load_dataset function with the appropriate path\n",
    "# 2) assign the result to a data matrix named X\n",
    "# BEGIN YOUR CODE\n",
    "X = load_dataset(\"../../../\")\n",
    "# END YOUR CODE\n",
    "print(\"A matrix with {0} images and {1} features per image was loaded.\".format(*X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: fill in this section to accomplish the following.\n",
    "# 1) call the show_image function using a single row from the matrix X\n",
    "# BEGIN YOUR CODE\n",
    "show_image(X[0, :], 32, 32)\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Two: Principal Component Analysis\n",
    "\n",
    "In this section, you will learn about Principal Component Analysis from an optimization perspective. You will then implement PCA to learn the $K$ principal components from the data matrix $X$. You will then use these principal components to interpolate between random rows of X. Finally, you will sample points in a lower dimensional subspace and invert PCA to generate new images of faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows of $X$ lives in the space of $\\mathcal{R}^{D}$. We define $D$ to be 3072 for the remainder of this homework. The principal components of $X$ provide a sequence of the best linear approximations to X in a lower dimensional subspace $\\mathcal{R}^{Q}$ where the rank of the subspace $Q \\leq D$ is no larger than the rank of the space that contains $X$. Consider a function of a vector $\\lambda$ in $\\mathcal{R}^{Q}$.\n",
    "\n",
    "$$ f(\\lambda) = \\mu + V_{Q} \\lambda $$\n",
    "\n",
    "This function defines a linear transformation from the space of $\\mathcal{R}^{Q}$ to the space of $\\mathcal{R}^{D}$. There are two important paraneters in this formulation: namely $\\mu$ and $V_{Q}$. The vector $\\mu$ is a position in the space of $\\mathcal{R}^{D}$. The matrix $V_{Q} \\in \\mathcal{R}^{D \\times Q}$ is a unitary matrix that maps the vector $\\lambda$ from the subspace $\\mathcal{R}^{Q}$ to the space of the data $\\mathcal{R}^{D}$. The goal of PCA is to minimize the following reconstruction error.\n",
    "\n",
    "$$ \\min_{\\mu, V_{Q}, \\{ \\lambda_{i}\\} } \\sum_{i = 1}^{N} || x_{i} - \\mu - V_{Q} \\lambda_{i} ||_{2}^{2} $$\n",
    "\n",
    "Where the vector $x_{i}$ is the row in position i from the data matrix $X$, and the vector $\\lambda_{i}$ represent the best approximation of the vector $x_{i}$ in the column space of the matrix $V_{Q}$. The other parameters have been previously defined, and are the same. We take this objective, and we optimize for $\\mu$ and $\\{ \\lambda_{i}\\}$.\n",
    "\n",
    "$$ \\mu = \\frac{1}{N} \\sum_{i = 1}^{N} x_{i} $$\n",
    "$$ \\lambda_{i} = V_{Q}^{T} ( x_{i} - \\mu ) $$\n",
    "\n",
    "The optimization objective now ammounts to solving for the optimal orthonormal matrix $V_{Q}$ that minimizes reconstruction error.\n",
    "\n",
    "$$ \\min_{V_{Q}} \\sum_{i = 1}^{N} || x_{i} - \\frac{1}{N} \\sum_{i = 1}^{N} x_{i} - V_{Q} V_{Q}^{T} ( x_{i} - \\frac{1}{N} \\sum_{i = 1}^{N} x_{i} ) ||_{2}^{2} $$\n",
    "\n",
    "The matrix resulting from $V_{Q} V_{Q}^{T}$ can be imagined a projection that maps each data point $x_{i}$ onto the best rank $Q$ approximation. See that we are subtracting the mean from each data point. If we assume each data point already has zero mean, the objective simplifies.\n",
    "\n",
    "$$ \\frac{1}{N} \\sum_{i = 1}^{N} x_{i} = 0 \\implies \\min_{V_{Q}} \\sum_{i = 1}^{N} || x_{i} - V_{Q} V_{Q}^{T} x_{i} ||_{2}^{2} $$\n",
    "\n",
    "The solution may be obtained using Singular Value Decomposition. In particular, we can express the data matrix by its SVD $X = U \\Sigma V^{T}$. Here, $U$ is an $N \\times D$ orthogonal matrix. The matrix $U \\Sigma$ represents the principal components of $X$, the directions with highest variance. The solution for $V_{Q}$ is simply to take the first $Q$ columns of the matrix $V$. This is left as an exercise for the reader and is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V_T = None, None, None\n",
    "# TODO: fill in this section to accomplish the following.\n",
    "# 1) using the function np.linalg.svd, calculate the singular value decomposition of the data matrix X\n",
    "# 2) assign the SVD results to three matrices: U, S, V_T\n",
    "# BEGIN YOUR CODE\n",
    "U, S, V_T = np.linalg.svd(X)\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variances along the first 256 principal components are: [1400.55737671  896.74130096  557.65750875  501.15021958  442.82439485\n",
      "  382.53117989  364.01446985  348.38646398  288.48813702  279.92680562\n",
      "  270.32482819  254.73803428  246.78940925  221.55841311  211.09873657\n",
      "  207.30438793  192.22276195  184.38524853  178.56125891  173.49592847\n",
      "  169.65656769  154.73954172  151.64613316  148.36561694  141.95923879\n",
      "  137.01974824  135.41728677  132.26052584  128.79801518  127.93181276\n",
      "  125.73132927  124.39058534  123.11059582  122.62173192  118.41125216\n",
      "  117.37436435  113.21144638  111.51583421  109.8207836   107.64045905\n",
      "  105.3906873   103.96102544  101.59940159  100.38176391   99.54615961\n",
      "   96.29722762   95.14088336   93.62961689   93.05066198   91.03197108\n",
      "   90.29209129   89.71981772   88.88537842   88.12777693   86.78812425\n",
      "   85.69469329   84.84581543   82.55814159   82.15712737   80.59977135\n",
      "   80.23039139   79.30100909   78.13240358   77.66356002   76.72772974\n",
      "   76.37680045   75.4503283    75.22642555   74.76152334   73.96846819\n",
      "   72.92409417   71.71425737   71.24442162   70.34888919   70.07657542\n",
      "   69.2442398    68.97439748   67.48941443   67.14646528   66.68979061\n",
      "   66.06534324   65.74589563   65.02738302   64.75778201   64.5061308\n",
      "   64.1176258    63.63096719   62.90811164   62.67085623   62.01301567\n",
      "   61.45193504   61.00554716   60.77703798   60.15038007   59.84873226\n",
      "   59.71813122   59.31963683   59.21085266   59.03585432   58.36907542\n",
      "   58.31149338   57.65690935   57.29694094   57.06310428   56.76505818\n",
      "   56.47318311   56.00341334   55.76586661   55.34248805   54.94440029\n",
      "   54.68524196   54.06122328   53.81596735   53.70287446   53.59001762\n",
      "   52.82027835   52.53878046   52.2933182    52.02578924   51.76517921\n",
      "   51.48596108   51.0019337    50.75725792   50.40550752   50.12104287\n",
      "   50.03243055   49.72469028   49.537292     49.37392267   48.9487959\n",
      "   48.73437156   48.39541648   48.24356264   48.00656987   47.95569934\n",
      "   47.86867135   47.71614418   47.22000306   47.09244438   46.85009286\n",
      "   46.64076946   46.45191521   46.22390699   46.04853162   46.02549979\n",
      "   45.67685753   45.55243379   45.4919194    45.42833863   45.2267845\n",
      "   45.11584668   44.71962152   44.60200801   44.49740602   44.06416716\n",
      "   43.9982825    43.78529714   43.62275585   43.48975826   43.43724865\n",
      "   43.31614434   43.11995457   42.91931443   42.73932874   42.51505249\n",
      "   42.32092987   42.24737235   41.96666475   41.86205125   41.66191919\n",
      "   41.55600122   41.49819609   41.21777172   41.16262621   40.99666257\n",
      "   40.86379325   40.64908291   40.5975648    40.43009981   40.34213716\n",
      "   40.29005731   40.14774088   39.86050981   39.75886371   39.64497284\n",
      "   39.4987995    39.34244169   39.12218593   39.04584516   38.9363649\n",
      "   38.84855956   38.78490397   38.71493552   38.56567297   38.46627392\n",
      "   38.32010887   38.2446468    38.06550364   37.97912776   37.75325999\n",
      "   37.63572209   37.59254761   37.49290098   37.37314722   37.23971048\n",
      "   37.20872521   37.11459285   37.06059604   36.8779127    36.79968015\n",
      "   36.59553797   36.5631193    36.43587895   36.41492915   36.25114555\n",
      "   36.06068791   36.02328748   35.89576821   35.82648137   35.79249474\n",
      "   35.6144634    35.56024876   35.51071032   35.41482292   35.35955428\n",
      "   35.22834537   35.0510744    35.04604992   34.89491315   34.69751856\n",
      "   34.65268582   34.55732852   34.47449104   34.37974481   34.28655932\n",
      "   34.19818687   34.12867662   34.06032789   33.97840653   33.95071739\n",
      "   33.83096985   33.74941925   33.69478451   33.56523859   33.54216975\n",
      "   33.40844711   33.33533572   33.24441573   33.19920686   33.09058502\n",
      "   32.99987398   32.95740936   32.87416638   32.81164536   32.75117804\n",
      "   32.66483973]\n",
      "The first 256 principal components are: [[-0.00241149  0.02039855 -0.00051158 ... -0.00225114  0.00311229\n",
      "   0.00026858]\n",
      " [ 0.00755859 -0.00089658 -0.00295127 ...  0.02482625 -0.0072695\n",
      "  -0.00071968]\n",
      " [-0.00395179  0.01228788  0.01545751 ...  0.02437975 -0.01661624\n",
      "  -0.01709547]\n",
      " ...\n",
      " [ 0.00695771  0.01512255  0.00899533 ... -0.00262146 -0.04418589\n",
      "  -0.00497446]\n",
      " [-0.0107659   0.01964406  0.00433874 ...  0.00746583  0.00965013\n",
      "  -0.00035394]\n",
      " [-0.00770392  0.0141866   0.0569132  ... -0.0117921  -0.00184641\n",
      "  -0.01835487]]\n",
      "The first 256 right singular vectors are: [[-0.0235451   0.02491075 -0.00778631 ... -0.00526038 -0.03115106\n",
      "   0.01724685]\n",
      " [-0.02558246  0.02220271 -0.00790224 ... -0.01647977 -0.01528896\n",
      "   0.01396448]\n",
      " [-0.02627786  0.01993618 -0.00835553 ... -0.00035036 -0.00669726\n",
      "   0.01252301]\n",
      " ...\n",
      " [-0.01202036 -0.00445318  0.01413568 ... -0.02076021  0.01927787\n",
      "   0.00502307]\n",
      " [-0.01443819 -0.01250724  0.0147313  ... -0.02400273  0.00429011\n",
      "   0.00197001]\n",
      " [-0.01521917 -0.01627267  0.01453407 ... -0.02037046 -0.00222976\n",
      "  -0.00332972]]\n"
     ]
    }
   ],
   "source": [
    "Q = 256\n",
    "principal_components = None\n",
    "V_Q = None\n",
    "# TODO: fill in this section to accomplish the following.\n",
    "# 1) define Q = 256 to be the rank of the lower dimensional subspace in which you shall embed the data points\n",
    "# 2) define variances to be the first Q singular values\n",
    "# 3) define principal_components to be the basis vectors corresponding to the first Q singular values\n",
    "# 4) define V_Q to be the matrix consisting of the first Q columns of V\n",
    "# BEGIN YOUR CODE\n",
    "\n",
    "variances = S[:Q] \n",
    "principal_components = U[:Q] #get basis vectors \n",
    "V = V_T.transpose()\n",
    "V_Q = V[:, :Q]\n",
    "# END YOUR CODE\n",
    "\n",
    "print(\"The variances along the first {0} principal components are: {1}\".format(Q, variances))\n",
    "print(\"The first {0} principal components are: {1}\".format(Q, principal_components))\n",
    "print(\"The first {0} right singular vectors are: {1}\".format(Q, V_Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in this section to accomplish the following.\n",
    "# 1) select a single row of the data matrix X\n",
    "# 2) project that row onto the rank-Q lower dimension subspace in R^D specified by the projection matrix (V_Q V_Q^T)\n",
    "# BEGIN YOUR CODE\n",
    "row0 = X[0,:]\n",
    "def projPCA(row):\n",
    "    multiplied_matrices = np.matmul(V_Q, V_Q.transpose())\n",
    "    projection = np.dot(multiplied_matrices, row)\n",
    "    return projection\n",
    "projection = projPCA(row0)\n",
    "\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: fill in this section to accomplish the following.\n",
    "# 1) display the original image using show_image with height 32 and width 32\n",
    "# 1) display the projected image using show_image with height 32 and width 32\n",
    "# BEGIN YOUR CODE\n",
    "show_image(X[0,:], 32, 32)\n",
    "show_image(projection, 32, 32)\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on how well the best Q principal components reconstruct the image:\n",
    "[TODO: your response here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_interpolation(z_one, z_two, reconstruction_function, output_height, output_width):\n",
    "    \"\"\"This function draws an interpolating animation from one image to another image in the latent space.\n",
    "    Args:\n",
    "        z_one: an np.float32 vector with Q elements.\n",
    "        z_two: an np.float32 vector with Q elements.\n",
    "        reconstruction_function: a function that takes in z_one or z_two and returns an np.float32 vector with D elements.\n",
    "        output_height: integer, the height to reshape each image to.\n",
    "        output_width: integer, the width to reshape each image to.\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    im = None\n",
    "    im = plt.imshow(reconstruction_function(z_one).reshape([output_height, output_width, 3]) / 2.0 + 0.5, animated=True)\n",
    "    def updatefig(t):\n",
    "        alpha = (0.5 * np.cos(t) + 0.5)\n",
    "        im.set_array(reconstruction_function(z_one * alpha + z_two * (1.0 - alpha)).reshape([output_height, output_width, 3]) / 2.0 + 0.5)\n",
    "        return im,\n",
    "    display(HTML(animation.FuncAnimation(fig, updatefig, frames=np.linspace(0, 2*np.pi, 64), blit=True).to_html5_video()))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Requested MovieWriter (ffmpeg) not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\vamsh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\animation.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ffmpeg'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-a72567baddcf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mreconstructed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mlatent_interpolation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproj1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproj2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstruction_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;31m# END YOUR CODE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-283ffb3341bb>\u001b[0m in \u001b[0;36mlatent_interpolation\u001b[1;34m(z_one, z_two, reconstruction_function, output_height, output_width)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreconstruction_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_one\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mz_two\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2.0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHTML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manimation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFuncAnimation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdatefig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_html5_video\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vamsh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\animation.py\u001b[0m in \u001b[0;36mto_html5_video\u001b[1;34m(self, embed_limit)\u001b[0m\n\u001b[0;32m   1337\u001b[0m                 \u001b[1;31m# We create a writer manually so that we can get the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m                 \u001b[1;31m# appropriate size for the tag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1339\u001b[1;33m                 \u001b[0mWriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwriters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'animation.writer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1340\u001b[0m                 writer = Writer(codec='h264',\n\u001b[0;32m   1341\u001b[0m                                 \u001b[0mbitrate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'animation.bitrate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vamsh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\animation.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             raise RuntimeError(\n\u001b[1;32m--> 164\u001b[1;33m                 'Requested MovieWriter ({}) not available'.format(name))\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Requested MovieWriter (ffmpeg) not available"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: fill in this section to accomplish the following.\n",
    "# 1) select two different rows from the data matrix X\n",
    "# 2) project each row onto the best Q principal components using V_Q^T\n",
    "# 3) define reconstruction_function that reconstructs a data point x from its projection onto the best Q principal components using V_Q\n",
    "# 4) call the function latent_interpolation and generate a visualization with height 32 and width 32 \n",
    "# BEGIN YOUR CODE\n",
    "testRow1 = X[1234, :]\n",
    "testRow2 = X[123, :]\n",
    "\n",
    "proj1 = projPCA(testRow1)\n",
    "proj2 = projPCA(testRow2)\n",
    "\n",
    "def reconstruction_function(projection):\n",
    "    multiplied_matrices = np.matmul(V_Q, V_Q.transpose())\n",
    "    reconstructed = np.dot(multiplied_matrices, projection)\n",
    "    return reconstructed\n",
    "    \n",
    "latent_interpolation(proj1, proj2, reconstruction_function, 32, 32)\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on what happens during the interpolation process:\n",
    "[TODO: your response here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_generation(z_mean, z_stddev, reconstruction_function, output_height, output_width):\n",
    "    \"\"\"This function samples from the latent space of the model and shows the resulting image.\n",
    "    Args:\n",
    "        z_mean: an np.float32 vector with Q elements.\n",
    "        z_stddev: an np.float32 matrix with Q by Q elements.\n",
    "        reconstruction_function: a function that takes in z_one or z_two and returns an np.float32 vector with D elements.\n",
    "        output_height: integer, the height to reshape each image to.\n",
    "        output_width: integer, the width to reshape each image to.\n",
    "    \"\"\"\n",
    "    sampled_point = z_mean + z_stddev.dot(np.random.normal(0, 1, z_mean.shape))\n",
    "    show_image(reconstruction_function(sampled_point), output_height, output_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in this section to accomplish the following.\n",
    "# 1) project the data matrix onto best Q principal components using V_Q^T\n",
    "# 2) compute z_mean as the average of the projected matrix along the 0th axis\n",
    "# 3) define z_stddev to be the rank Q identity matrix for now\n",
    "# 4) generate an new image by calling latent_generation with height 32 and width 32 \n",
    "# BEGIN YOUR CODE\n",
    "\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on how real the generated face looks:\n",
    "[TODO: your response here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Three: Linear Autoencoder\n",
    "\n",
    "In this section, you will learn about the linear autoencoder, and you will also implement the linear autoencoder using pytorch. We shall use your linear autoencoder to interpolate between data points from $X$ and to also generate new samples of faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, image_height, image_width, hidden_size):\n",
    "        \"\"\"Creates a single layer neural network.\n",
    "        Args:\n",
    "            image_height: an integer, the height of each image\n",
    "            image_width: an integer, the width of each image\n",
    "            hidden_size: an integer, the number of neurons in the hidden layer of this network\n",
    "        \"\"\"\n",
    "        super(LinearEncoder, self).__init__()\n",
    "        # TODO: fill in this section to accomplish the following.\n",
    "        # 1) create a single layer neural network that performs a linear transformation from a vector with \n",
    "        #    image_height * image_width * 3 dimensions to a vector with hidden_size dimensions.\n",
    "        #    HINT: consider the class torch.nn.Linear\n",
    "        # BEGIN YOUR CODE\n",
    "        # END YOUR CODE\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes a single forward pass of this network.\n",
    "        Args:\n",
    "            x: a float32 tensor with shape [batch_size, D]\n",
    "        Returns:\n",
    "            a float32 tensor with shape [batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        # TODO: fill in this section to accomplish the following.\n",
    "        # 1) perform a forward pass using the hidden layer you defined\n",
    "        # 2) return the resulting vector\n",
    "        # BEGIN YOUR CODE\n",
    "        # END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, image_height, image_width, hidden_size):\n",
    "        \"\"\"Creates a single layer neural network.\n",
    "        Args:\n",
    "            image_height: an integer, the height of each image\n",
    "            image_width: an integer, the width of each image\n",
    "            hidden_size: an integer, the number of neurons in the hidden layer of this network\n",
    "        \"\"\"\n",
    "        super(LinearDecoder, self).__init__()\n",
    "        # TODO: fill in this section to accomplish the following.\n",
    "        # 1) create a single layer neural network that performs a linear transformation from a vector with \n",
    "        #    hidden_size dimensions to a vector with image_height * image_width * 3 dimensions.\n",
    "        #    HINT: consider the class torch.nn.Linear\n",
    "        # BEGIN YOUR CODE\n",
    "        # END YOUR CODE\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes a single forward pass of this network.\n",
    "        Args:\n",
    "            x: a float32 tensor with shape [batch_size, hidden_size]\n",
    "        Returns:\n",
    "            a float32 tensor with shape [batch_size, D]\n",
    "        \"\"\"\n",
    "        # TODO: fill in this section to accomplish the following.\n",
    "        # 1) perform a forward pass using the hidden layer you defined\n",
    "        # 2) return the resulting vector\n",
    "        # BEGIN YOUR CODE\n",
    "        # END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_encoder = None\n",
    "linear_decoder = None\n",
    "linear_autoencoder_loss = None\n",
    "linear_autoencoder_optimizer = None\n",
    "# TODO: fill in this section to accomplish the following.\n",
    "# 1) create an instance of LinearEncoder named linear_encoder with height 32, width 32, and hidden size Q\n",
    "# 1) create an instance of LinearDecoder named linear_decoder with height 32, width 32, and hidden size Q\n",
    "# 2) assign linear_autoencoder_loss to be an instance of torch.nn.MSELoss\n",
    "# 2) create an optimizer named linear_autoencoder_optimizer of your choosing with a learning rate of your choosing.\n",
    "#    HINT: consider the torch.optim.Adam object\n",
    "# BEGIN YOUR CODE\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: run the following section of code in order to train the model\n",
    "# Construct a tensor from the dataset\n",
    "image_tensor = torch.FloatTensor(X)\n",
    "for i in range(1000):\n",
    "    # Clear the previous gradient from the optimizer by calling .zero_grad()\n",
    "    linear_autoencoder_optimizer.zero_grad()\n",
    "    # Compute a full encoding and decoding step\n",
    "    reconstructed_image = linear_decoder(linear_encoder(image_tensor))\n",
    "    # Compute the mean squared reconstruction loss\n",
    "    loss = linear_autoencoder_loss(reconstructed_image, image_tensor)\n",
    "    # Pass the loss backward throgh the network, and compute the gradients\n",
    "    loss.backward()\n",
    "    # Update the optimizer by calling .step()\n",
    "    linear_autoencoder_optimizer.step()\n",
    "    # Return a detatched value of the loss for logging purposes\n",
    "    print(\"On iteration {0} the loss was {1}.\".format(i, loss.detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run the following section of code to define the reconstruction function\n",
    "def linear_autoencoder_reconstruction_function(z):\n",
    "    return linear_decoder(torch.FloatTensor(z[np.newaxis, :])).detach()[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: fill in this section to accomplish the following.\n",
    "# 1) select two different rows from the data matrix X\n",
    "# 2) compute the hidden representation for each row using your linear_encoder\n",
    "# 3) call the function latent_interpolation and generate a visualization with height 32 and width 32 \n",
    "# BEGIN YOUR CODE\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on what happens during the interpolation process:\n",
    "[TODO: your response here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: fill in this section to accomplish the following.\n",
    "# 1) compute the hidden representation of each row of the data matrix\n",
    "# 2) compute z_mean as the average of the hidden representation matrix along the 0th axis\n",
    "# 3) define z_stddev to be the rank Q identity matrix for now\n",
    "# 4) generate an new image by calling latent_generation with height 32 and width 32 \n",
    "# BEGIN YOUR CODE\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on how real the generated face looks:\n",
    "[TODO: your response here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on how the linear autoencoder compares to PCA:\n",
    "[TODO: your response here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Four: Convolutional Autoencoder\n",
    "\n",
    "In this section, you will learn about the convolutional autoencoder, and you will also implement the convolutional autoencoder using pytorch. We shall use your convolutional autoencoder to interpolate between data points from $X$ and to also generate new samples of faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, image_height, image_width, final_size):\n",
    "        \"\"\"Creates a deep convolutional neural network.\n",
    "        Args:\n",
    "            image_height: an integer, the height of each image\n",
    "            image_width: an integer, the width of each image\n",
    "            final_size: an integer, the depth of the final layer of this network\n",
    "        \"\"\"\n",
    "        super(ConvolutionalEncoder, self).__init__()\n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "        self.final_size = max(16 * 3, final_size)\n",
    "        # TODO: fill in this section to accomplish the following.\n",
    "        # 1) create a 5 layer convolutional neural network that transforms an image with shape\n",
    "        #    [image_height, image_width, 3] to a vector with final_size dimensions.\n",
    "        #    HINT: consider the class torch.nn.Conv2d with stride=2\n",
    "        # BEGIN YOUR CODE\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=8,\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            in_channels=8,\n",
    "            out_channels=64,\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.conv3 = torch.nn.Conv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=128,\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "        self.conv4 = torch.nn.Conv2d(\n",
    "            in_channels=128,\n",
    "            out_channels=164,\n",
    "            kernel_size=2,\n",
    "            stride=2,\n",
    "        )\n",
    "\n",
    "        self.conv5 = torch.nn.Conv2d(\n",
    "            in_channels=164,\n",
    "            out_channels=256,\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "        \n",
    "        #self.output_shape = (int(self.image_height/2/2), int(self.image_width/2/2), 1)\n",
    "        #self.fc1 = torch.nn.Linear(np.prod(self.output_shape), 128)\n",
    "        #self.output_fc = torch.nn.Linear(128, self.final_size)\n",
    "        \n",
    "        # END YOUR CODE\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes a single forward pass of this network.\n",
    "        Args:\n",
    "            x: a float32 tensor with shape [batch_size, D]\n",
    "        Returns:\n",
    "            a float32 tensor with shape [batch_size, final_size]\n",
    "        \"\"\"\n",
    "        x = x.view(x.size()[0], self.image_height, self.image_width, 3)\n",
    "        x = torch.transpose(x, 1, 3)\n",
    "        # TODO: fill in this section to accomplish the following.\n",
    "        # 1) perform a forward pass using the conv layers you defined\n",
    "        # 2) apply any activation function you want\n",
    "        # BEGIN YOUR CODE\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.conv3(x))\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.conv4(x))\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.conv5(x))\n",
    "        #print(x.size())\n",
    "        \n",
    "        #x = F.relu(self.fc1(x))\n",
    "        #x = self.output_fc(x)\n",
    "        # END YOUR CODE\n",
    "        x = torch.transpose(x, 1, 3)\n",
    "        x = x.contiguous()\n",
    "        #encoder\n",
    "        x = x.view(x.size()[0], self.final_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalDecoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, image_height, image_width, final_size):\n",
    "        \"\"\"Creates a deep convolutional neural network.\n",
    "        Args:\n",
    "            image_height: an integer, the height of each image\n",
    "            image_width: an integer, the width of each image\n",
    "            final_size: an integer, the depth of the final layer of this network\n",
    "        \"\"\"\n",
    "        super(ConvolutionalDecoder, self).__init__()\n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "        self.final_size = max(16 * 3, final_size)\n",
    "        # TODO: fill in this section to accomplish the following.\n",
    "        # 1) create a 5 layer transpose convolutional neural network that transforms a vector with \n",
    "        #    final_size dimensions to an image with shape [image_height, image_width, 3]\n",
    "        #    HINT: consider the class torch.nn.ConvTranspose2d with stride=2\n",
    "        # BEGIN YOUR CODE\n",
    "        self.conv1 = torch.nn.ConvTranspose2d(\n",
    "            in_channels=256,\n",
    "            out_channels=164,\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "        \n",
    "        self.conv2 = torch.nn.ConvTranspose2d(\n",
    "            in_channels=164,\n",
    "            out_channels=128,\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "        \n",
    "        self.conv3 = torch.nn.ConvTranspose2d(\n",
    "            in_channels=128,\n",
    "            out_channels=64,\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.conv4 = torch.nn.ConvTranspose2d(\n",
    "            in_channels=64,\n",
    "            out_channels=8,\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "        \n",
    "        self.conv5 = torch.nn.ConvTranspose2d(\n",
    "            in_channels=8,\n",
    "            out_channels=3,\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "    \n",
    "        # END YOUR CODE\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes a single forward pass of this network.\n",
    "        Args:\n",
    "            x: a float32 tensor with shape [batch_size, final_size]\n",
    "        Returns:\n",
    "            a float32 tensor with shape [batch_size, D]\n",
    "        \"\"\"\n",
    "        x = x.view(x.size()[0], 1, 1, self.final_size)\n",
    "        x = torch.transpose(x, 1, 3)\n",
    "        # TODO: fill in this section to accomplish the following.\n",
    "        # 1) perform a forward pass using the conv layers you defined\n",
    "        # 2) apply any activation function you want\n",
    "        # BEGIN YOUR CODE\n",
    "        #print(\"decoder start\")\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.conv3(x))\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.conv4(x))\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.conv5(x))\n",
    "        #print(x.size())\n",
    "        \n",
    "        # END YOUR CODE\n",
    "        x = torch.transpose(x, 1, 3)\n",
    "        x = x.contiguous()\n",
    "        x = x.view(x.size()[0], self.image_height * self.image_width * 3)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolutional_encoder = None\n",
    "convolutional_decoder = None\n",
    "convolutional_autoencoder_loss = None\n",
    "convolutional_autoencoder_optimizer = None\n",
    "# TODO: fill in this section to accomplish the following.\n",
    "# 1) create an instance of ConvolutionalEncoder named convolutional_encoder with height 32, width 32, and hidden size Q\n",
    "# 1) create an instance of ConvolutionalDecoder named convolutional_decoder with height 32, width 32, and hidden size Q\n",
    "# 2) assign convolutional_autoencoder_loss to be an instance of torch.nn.MSELoss\n",
    "# 2) create an optimizer named convolutional_autoencoder_optimizer of your choosing with a learning rate of your choosing.\n",
    "#    HINT: consider the torch.optim.Adam object\n",
    "# BEGIN YOUR CODE\n",
    "convolutional_encoder = ConvolutionalEncoder(32, 32, Q)\n",
    "convolutional_decoder = ConvolutionalDecoder(32, 32, Q)\n",
    "convolutional_autoencoder_loss = torch.nn.MSELoss()\n",
    "convolutional_autoencoder_optimizer = torch.optim.Adam(convolutional_encoder.parameters(), lr=0.1)\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99212599,  0.81889766,  0.52755904, ..., -0.13385826,\n",
       "        -0.65354329, -0.85826772],\n",
       "       [-0.48818898, -0.60629922, -0.66141731, ...,  0.02362205,\n",
       "        -0.20472442, -0.23622048],\n",
       "       [ 1.00787401,  1.00787401,  1.00787401, ..., -0.26771653,\n",
       "        -0.28346458, -0.25984251],\n",
       "       ...,\n",
       "       [ 0.38582677,  0.50393701,  0.48031497, ..., -0.55905509,\n",
       "        -0.82677168, -0.83464569],\n",
       "       [-0.48031497, -0.66929132, -0.70078743, ...,  0.02362205,\n",
       "         0.11811024, -0.24409449],\n",
       "       [ 0.85826772,  0.89763778,  0.92913383, ..., -0.72440946,\n",
       "        -0.73228347, -0.7480315 ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-b5225a39863c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvolutional_autoencoder_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreconstructed_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Pass the loss backward throgh the network, and compute the gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Update the optimizer by calling .step()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mconvolutional_autoencoder_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vamsh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vamsh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: run the following section of code in order to train the model\n",
    "# Construct a tensor from the dataset\n",
    "image_tensor = torch.FloatTensor(X)\n",
    "for i in range(1000):\n",
    "    # Clear the previous gradient from the optimizer by calling .zero_grad()\n",
    "    convolutional_autoencoder_optimizer.zero_grad()\n",
    "    # Compute a full encoding and decoding step\n",
    "    reconstructed_image = convolutional_decoder(convolutional_encoder(image_tensor))\n",
    "    # Compute the mean squared reconstruction loss\n",
    "    loss = convolutional_autoencoder_loss(reconstructed_image, image_tensor)\n",
    "    # Pass the loss backward throgh the network, and compute the gradients\n",
    "    loss.backward()\n",
    "    # Update the optimizer by calling .step()\n",
    "    convolutional_autoencoder_optimizer.step()\n",
    "    # Return a detatched value of the loss for logging purposes\n",
    "    print(\"On iteration {0} the loss was {1}.\".format(i, loss.detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run the following section of code to define the reconstruction function\n",
    "def convolutional_autoencoder_reconstruction_function(z):\n",
    "    return np.asarray(convolutional_decoder(torch.FloatTensor(z[np.newaxis, :])).detach()[0, :], np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot find a common data type.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-d8cf42f059ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mrow2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvolutional_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mlatent_interpolation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprojPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvolutional_autoencoder_reconstruction_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;31m# END YOUR CODE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-4ba83a05184f>\u001b[0m in \u001b[0;36mprojPCA\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprojPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmultiplied_matrices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mV_Q\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV_Q\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mprojection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmultiplied_matrices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mprojection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprojection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprojPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot find a common data type."
     ]
    }
   ],
   "source": [
    "# TODO: fill in this section to accomplish the following.\n",
    "# 1) select two different rows from the data matrix X\n",
    "# 2) compute the hidden representation for each row using your convolutional_encoder\n",
    "# 3) call the function latent_interpolation and generate a visualization with height 32 and width 32 \n",
    "# BEGIN YOUR CODE\n",
    "image = torch.FloatTensor(X)\n",
    "row1 = convolutional_encoder(image)[0]\n",
    "row2 = convolutional_encoder(image)[1]\n",
    "\n",
    "latent_interpolation(projPCA(row1), projPCA(row2), convolutional_autoencoder_reconstruction_function, 32, 32)\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on what happens during the interpolation process:\n",
    "[TODO: your response here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in this section to accomplish the following.\n",
    "# 1) compute the hidden representation of each row of the data matrix using convolutional_encoder\n",
    "# 2) compute z_mean as the average of the hidden representation matrix along the 0th axis\n",
    "# 3) define z_stddev to be the rank Q identity matrix for now\n",
    "# 4) generate an new image by calling latent_generation with height 32 and width 32 \n",
    "# BEGIN YOUR CODE\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on how real the generated face looks:\n",
    "[TODO: your response here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on how the convolutional autoencoder compares to the linear autoencoder and PCA:\n",
    "[TODO: your response here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Section Five: Variational Autoencoder\n",
    "\n",
    "In this section, we implement the Variational Autoencoder, an extension for the traditional autoencoder that explicitly models the probability distribution of a latent variable. This section is optional, and so we fill in the code for you. If you have extra time after completing the rest of this homework, you should first read this tutorial on variational inference https://arxiv.org/pdf/1606.05908.pdf. Then, you may attempt to train the VAE given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run the following section of code\n",
    "class Sampler(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        \"\"\"Creates a Variational sampling layer.\n",
    "        Args:\n",
    "            hidden_size: an integer, the number of neurons in the sampling layer.\n",
    "        \"\"\"\n",
    "        super(Sampler, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.log_scale = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.shift = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes a single forward pass of this network.\n",
    "        Args:\n",
    "            x: a float32 tensor with shape [batch_size, hidden_size]\n",
    "        Returns:\n",
    "            a float32 tensor with shape [batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        scale = torch.exp(self.log_scale(x))\n",
    "        shift = self.shift(x)\n",
    "        sample = torch.randn([self.hidden_size]) * scale + shift\n",
    "        return sample\n",
    "        \n",
    "    def kl_penalty(self, x):\n",
    "        \"\"\"Computes a single forward pass of this network.\n",
    "        Args:\n",
    "            x: a float32 tensor with shape [batch_size, hidden_size]\n",
    "        Returns:\n",
    "            a float32 scalar: KL divergence between this distribution and the standard normal distribution.\n",
    "        \"\"\"\n",
    "        log_scale = self.log_scale(x)\n",
    "        scale = torch.exp(log_scale)\n",
    "        shift = self.shift(x)\n",
    "        return torch.mean(log_scale + (1.0 + shift * shift) / (2.0 * scale * scale) - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run the following section of code\n",
    "variational_encoder = ConvolutionalEncoder(32, 32, 256)\n",
    "variational_decoder = ConvolutionalDecoder(32, 32, 256)\n",
    "sampler = Sampler(256)\n",
    "variational_autoencoder_loss = torch.nn.MSELoss()\n",
    "variational_autoencoder_optimizer = torch.optim.Adam([\n",
    "    {\"params\": variational_encoder.parameters()}, \n",
    "    {\"params\": variational_decoder.parameters()}, \n",
    "    {\"params\": sampler.parameters()}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: run the following section of code in order to train the model\n",
    "# Construct a tensor from the dataset\n",
    "image_tensor = torch.FloatTensor(X)\n",
    "for i in range(10000):\n",
    "    # Clear the previous gradient from the optimizer by calling .zero_grad()\n",
    "    variational_autoencoder_optimizer.zero_grad()\n",
    "    # Compute a full encoding and decoding step\n",
    "    hidden_variables = variational_encoder(image_tensor)\n",
    "    reconstructed_image = variational_decoder(sampler(hidden_variables))\n",
    "    # Compute the mean squared reconstruction loss\n",
    "    loss = variational_autoencoder_loss(reconstructed_image, image_tensor) - sampler.kl_penalty(hidden_variables)\n",
    "    # Pass the loss backward throgh the network, and compute the gradients\n",
    "    loss.backward()\n",
    "    # Update the optimizer by calling .step()\n",
    "    variational_autoencoder_optimizer.step()\n",
    "    # Return a detatched value of the loss for logging purposes\n",
    "    print(\"On iteration {0} the loss was {1}.\".format(i, loss.detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run the following section of code\n",
    "def variational_autoencoder_reconstruction_function(z):\n",
    "    return np.asarray(variational_decoder(torch.FloatTensor(z[np.newaxis, :])).detach()[0, :], np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run the following section of code\n",
    "x_one = X[0, :]\n",
    "x_two = X[1, :]\n",
    "z_one = np.asarray(sampler(variational_encoder(torch.FloatTensor(x_one[np.newaxis, :]))).detach(), np.float32)\n",
    "z_two = np.asarray(sampler(variational_encoder(torch.FloatTensor(x_two[np.newaxis, :]))).detach(), np.float32)\n",
    "latent_interpolation(z_one, z_two, convolutional_autoencoder_reconstruction_function, 32, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on what happens during the interpolation process:\n",
    "[TODO: your response here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run the following section of code\n",
    "z_mean = np.asarray(torch.mean(sampler(variational_encoder(torch.FloatTensor(X))), 0).detach(), np.float32)\n",
    "z_stddev = np.identity(Q)\n",
    "latent_generation(z_mean, z_stddev, convolutional_autoencoder_reconstruction_function, 32, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on how real the generated face looks:\n",
    "[TODO: your response here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on how the convolutional autoencoder compares to the linear autoencoder and PCA:\n",
    "[TODO: your response here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
