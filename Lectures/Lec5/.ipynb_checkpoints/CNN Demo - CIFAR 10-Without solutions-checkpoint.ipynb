{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN THIS CELL\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a transformation that changes are images to Tensors, \n",
    "# and then maps the images from the range [0, 1] to the range [-1, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    }
   ],
   "source": [
    "## RUN THIS CELL\n",
    "training_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "## RUN THIS CELL\n",
    "test_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(\n",
    "    training_set,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classes in our dataset\n",
    "classes = [\n",
    "    'plane',\n",
    "    'car',\n",
    "    'bird',\n",
    "    'cat',\n",
    "    'deer',\n",
    "    'dog',\n",
    "    'frog',\n",
    "    'horse',\n",
    "    'ship',\n",
    "    'truck',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## our network\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=8,\n",
    "            kernel_size=5,\n",
    "            padding=5,\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(\n",
    "            kernel_size=8,\n",
    "            stride=2,\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=8,\n",
    "            out_channels=16,\n",
    "            kernel_size=5,\n",
    "            padding=5,\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(\n",
    "            kernel_size=8,\n",
    "            stride=2,\n",
    "        )\n",
    "        self.output_shape = (int(32/2/2), int(32/2/2), 16)\n",
    "        self.fc1 = nn.Linear(np.prod(self.output_shape), 128)\n",
    "        self.output_fc = nn.Linear(128, len(classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, np.prod(self.output_shape))\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.output_fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "PRINT_FREQ = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,0] loss: 0.023\n",
      "[0,100] loss: 2.290\n",
      "[0,200] loss: 2.276\n",
      "[0,300] loss: 2.222\n",
      "[0,400] loss: 2.124\n",
      "[0,500] loss: 2.056\n",
      "[0,600] loss: 2.079\n",
      "[0,700] loss: 2.038\n",
      "[0,800] loss: 2.021\n",
      "[0,900] loss: 1.945\n",
      "[0,1000] loss: 1.904\n",
      "[0,1100] loss: 1.925\n",
      "[0,1200] loss: 1.868\n",
      "[0,1300] loss: 1.936\n",
      "[0,1400] loss: 1.851\n",
      "[0,1500] loss: 1.887\n",
      "[0,1600] loss: 1.762\n",
      "[0,1700] loss: 1.860\n",
      "[0,1800] loss: 1.776\n",
      "[0,1900] loss: 1.825\n",
      "[0,2000] loss: 1.647\n",
      "[0,2100] loss: 1.728\n",
      "[0,2200] loss: 1.805\n",
      "[0,2300] loss: 1.766\n",
      "[0,2400] loss: 1.733\n",
      "[0,2500] loss: 1.724\n",
      "[0,2600] loss: 1.620\n",
      "[0,2700] loss: 1.720\n",
      "[0,2800] loss: 1.682\n",
      "[0,2900] loss: 1.531\n",
      "[0,3000] loss: 1.708\n",
      "[0,3100] loss: 1.759\n",
      "[0,3200] loss: 1.762\n",
      "[0,3300] loss: 1.723\n",
      "[0,3400] loss: 1.659\n",
      "[0,3500] loss: 1.669\n",
      "[0,3600] loss: 1.671\n",
      "[0,3700] loss: 1.676\n",
      "[0,3800] loss: 1.621\n",
      "[0,3900] loss: 1.636\n",
      "[0,4000] loss: 1.690\n",
      "[0,4100] loss: 1.611\n",
      "[0,4200] loss: 1.626\n",
      "[0,4300] loss: 1.678\n",
      "[0,4400] loss: 1.643\n",
      "[0,4500] loss: 1.592\n",
      "[0,4600] loss: 1.644\n",
      "[0,4700] loss: 1.654\n",
      "[0,4800] loss: 1.542\n",
      "[0,4900] loss: 1.527\n",
      "[0,5000] loss: 1.580\n",
      "[0,5100] loss: 1.603\n",
      "[0,5200] loss: 1.635\n",
      "[0,5300] loss: 1.551\n",
      "[0,5400] loss: 1.654\n",
      "[0,5500] loss: 1.556\n",
      "[0,5600] loss: 1.604\n",
      "[0,5700] loss: 1.528\n",
      "[0,5800] loss: 1.507\n",
      "[0,5900] loss: 1.584\n",
      "[0,6000] loss: 1.515\n",
      "[0,6100] loss: 1.546\n",
      "[0,6200] loss: 1.500\n",
      "[0,6300] loss: 1.538\n",
      "[0,6400] loss: 1.523\n",
      "[0,6500] loss: 1.433\n",
      "[0,6600] loss: 1.488\n",
      "[0,6700] loss: 1.627\n",
      "[0,6800] loss: 1.487\n",
      "[0,6900] loss: 1.563\n",
      "[0,7000] loss: 1.539\n",
      "[0,7100] loss: 1.553\n",
      "[0,7200] loss: 1.577\n",
      "[0,7300] loss: 1.508\n",
      "[0,7400] loss: 1.517\n",
      "[0,7500] loss: 1.482\n",
      "[0,7600] loss: 1.531\n",
      "[0,7700] loss: 1.550\n",
      "[0,7800] loss: 1.464\n",
      "[0,7900] loss: 1.565\n",
      "[0,8000] loss: 1.482\n",
      "[0,8100] loss: 1.558\n",
      "[0,8200] loss: 1.517\n",
      "[0,8300] loss: 1.525\n",
      "[0,8400] loss: 1.496\n",
      "[0,8500] loss: 1.590\n",
      "[0,8600] loss: 1.493\n",
      "[0,8700] loss: 1.542\n",
      "[0,8800] loss: 1.478\n",
      "[0,8900] loss: 1.520\n",
      "[0,9000] loss: 1.503\n",
      "[0,9100] loss: 1.463\n",
      "[0,9200] loss: 1.417\n",
      "[0,9300] loss: 1.442\n",
      "[0,9400] loss: 1.489\n",
      "[0,9500] loss: 1.419\n",
      "[0,9600] loss: 1.632\n",
      "[0,9700] loss: 1.527\n",
      "[0,9800] loss: 1.504\n",
      "[0,9900] loss: 1.497\n",
      "[0,10000] loss: 1.370\n",
      "[0,10100] loss: 1.474\n",
      "[0,10200] loss: 1.539\n",
      "[0,10300] loss: 1.518\n",
      "[0,10400] loss: 1.476\n",
      "[0,10500] loss: 1.494\n",
      "[0,10600] loss: 1.522\n",
      "[0,10700] loss: 1.389\n",
      "[0,10800] loss: 1.457\n",
      "[0,10900] loss: 1.434\n",
      "[0,11000] loss: 1.403\n",
      "[0,11100] loss: 1.490\n",
      "[0,11200] loss: 1.381\n",
      "[0,11300] loss: 1.553\n",
      "[0,11400] loss: 1.393\n",
      "[0,11500] loss: 1.434\n",
      "[0,11600] loss: 1.382\n",
      "[0,11700] loss: 1.435\n",
      "[0,11800] loss: 1.452\n",
      "[0,11900] loss: 1.458\n",
      "[0,12000] loss: 1.495\n",
      "[0,12100] loss: 1.455\n",
      "[0,12200] loss: 1.438\n",
      "[0,12300] loss: 1.425\n",
      "[0,12400] loss: 1.437\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    current_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(training_loader, 0):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        current_loss += loss.item()\n",
    "        if i % PRINT_FREQ == 0:\n",
    "            print(f'[{epoch},{i}] loss: {current_loss / PRINT_FREQ:.3f}')\n",
    "            current_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test network on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the 10000 test images: 47.78\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy on the 10000 test images: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Damn that sucks, what can we do better?\n",
    "\n",
    "### Answer: Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to use a pretrained imagenet model and transfer it to our CIFAR task.\n",
    "# First we need to recreate our datasets so that the input images to the network \"look like\"\n",
    "# imagenet images.\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize([224, 224]), #imagenet images are 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize( # pretrained network was trained with these params\n",
    "        mean=mean,\n",
    "        std=std, \n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "training_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(\n",
    "    training_set,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we pull in our pretrained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\vamsh/.torch\\models\\vgg16-397923af.pth\n",
      "553433881it [00:16, 34113899.29it/s]\n"
     ]
    }
   ],
   "source": [
    "## RUN THIS CELL\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to make it so that gradients don't get calculated for the parameters\n",
    "# in the pretrained model.\n",
    "for param in vgg16.parameters():\n",
    "    param.require_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace)\n",
       "  (2): Dropout(p=0.5)\n",
       "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (4): ReLU(inplace)\n",
       "  (5): Dropout(p=0.5)\n",
       "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to change the final fully connected layer in the model\n",
    "# since imagenet has 1000 classes and we only want to predict 10\n",
    "\n",
    "# get number of input features to last layer\n",
    "num_features = vgg16.classifier[6].in_features\n",
    "# overwrite existing fc with new one\n",
    "vgg16.classifier[6] = nn.Linear(num_features, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchsummary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-997f412bfd6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# look at a summary of our network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorchsummary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtorchsummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvgg16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchsummary'"
     ]
    }
   ],
   "source": [
    "# look at a summary of our network\n",
    "import torchsummary\n",
    "torchsummary.summary(vgg16, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we need to copy our code for the loss function, training, etc from above\n",
    "# NOTE: it would have been better practice to make that code into functions so that\n",
    "# we wouldn't have to copy and paste code around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                        | 0/12500 [00:00<?, ?it/s]\n",
      "loss: 0.121:   0%|                                                                           | 0/12500 [00:11<?, ?it/s]\n",
      "loss: 0.121:   0%|                                                                | 1/12500 [00:11<39:09:17, 11.28s/it]\n",
      "loss: 0.121:   0%|                                                                | 2/12500 [00:22<39:16:35, 11.31s/it]\n",
      "loss: 0.121:   0%|                                                                | 3/12500 [00:33<38:30:31, 11.09s/it]\n",
      "loss: 0.121:   0%|                                                                | 4/12500 [00:44<38:33:46, 11.11s/it]\n",
      "loss: 0.121:   0%|                                                                | 5/12500 [00:55<38:49:00, 11.18s/it]"
     ]
    }
   ],
   "source": [
    "PRINT_FREQ = 20\n",
    "from tqdm import tqdm\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    current_loss = 0.0\n",
    "    pbar = tqdm(enumerate(training_loader, 0), total=len(training_loader))\n",
    "    for i, data in pbar:\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vgg16(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        current_loss += loss.item()\n",
    "        if i % PRINT_FREQ == 0:\n",
    "            pbar.set_description(f'loss: {current_loss / PRINT_FREQ:.3f}')\n",
    "            current_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Model on Test data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = vgg16(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if total > 100:\n",
    "            break\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on the {total} test images: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "## Save the model\n",
    "torch.save(\n",
    "    {\n",
    "        'epoch': 1,\n",
    "        'state_dict': vgg16.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    },\n",
    "    'model.pth.tar'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "def load_model(filename):\n",
    "    checkpoint = torch.load(filename, map_location='cpu')\n",
    "    vgg16.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model('model.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(img):\n",
    "    print(img.shape)\n",
    "    npimg = img.numpy()\n",
    "    npimg = npimg * np.array(std).reshape((-1, 1, 1)) + np.array(mean).reshape((-1, 1, 1))\n",
    "    return np.transpose(npimg, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(inverse_transform(torchvision.utils.make_grid(images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(classes[label] for label in labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = vgg16(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted = torch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(classes[label] for label in predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
